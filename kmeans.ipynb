from sklearn.datasets import make_blobs
import pandas as pd
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt

dataset, classes = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=0.5, random_state=0)
# make as panda dataframe for easy understanding
df = pd.DataFrame(dataset, columns=['var1', 'var2'])

# Visualize the Elbow method to determine the optimal number of clusters
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12)).fit(df)
visualizer.show()
# Cluster the data using KMeans with 4 clusters
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(df)
# Print the cluster labels and cluster statistics
print(kmeans.labels_)
print(kmeans.inertia_)
print(kmeans.n_iter_)
print(kmeans.cluster_centers_)
print(Counter(kmeans.labels_))

# Load the dataset
df = sns.load_dataset('iris')[['sepal_length', 'sepal_width']]
# Determine the optimal number of clusters using the elbow method
model = KMeans()
# Fit the KMeans model with 3 clusters
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=0).fit(df)
# Plot the scatterplot with the predicted labels and centroids
sns.scatterplot(data=df, x="sepal_length", y="sepal_width", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],
marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()
